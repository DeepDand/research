2018-03-30 00:02:46.912465: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-30 00:02:46.912495: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-30 00:02:46.912517: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-30 00:02:46.912521: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-30 00:02:46.912525: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-30 00:02:48.519924: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties: 
name: Tesla P100-SXM2-16GB
major: 6 minor: 0 memoryClockRate (GHz) 1.4805
pciBusID 0000:04:00.0
Total memory: 15.89GiB
Free memory: 15.61GiB
2018-03-30 00:02:48.519969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 
2018-03-30 00:02:48.519975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y 
2018-03-30 00:02:48.519989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla P100-SXM2-16GB, pci bus id: 0000:04:00.0)
['S01']
[]
[]
Loading images  ../dataset/*.jpeg ['S01']
Number of images: 6200
Number of rows: 256
Number of cols: 256
Dataset array size:  (6200, 256, 256, 1)
(3100, 256, 256, 1)
(1550, 256, 256, 1)
(1550, 256, 256, 1)



| Training Step | Cross Entropy |  Layer  |   Epoch  |
|---------------|---------------|---------|----------|
|             0 |    46742.9102 | Layer 1 | Epoch 1  |
|          5000 |    24311.0586 | Layer 1 | Epoch 2  |
|         10000 |    22148.8574 | Layer 1 | Epoch 4  |
|         15000 |    20997.6582 | Layer 1 | Epoch 5  |
|         20000 |    20187.0312 | Layer 1 | Epoch 7  |
|         25000 |    19331.3691 | Layer 1 | Epoch 9  |
|         30000 |    18988.7422 | Layer 1 | Epoch 10  |
|         35000 |    18448.4922 | Layer 1 | Epoch 12  |
|         40000 |    17782.2676 | Layer 1 | Epoch 13  |
|         45000 |    17989.6465 | Layer 1 | Epoch 15  |
|         50000 |    17588.2441 | Layer 1 | Epoch 17  |
|         55000 |    17519.8672 | Layer 1 | Epoch 18  |
|         60000 |    17388.6914 | Layer 1 | Epoch 20  |
|         65000 |    17011.2188 | Layer 1 | Epoch 21  |
|         70000 |    16937.0566 | Layer 1 | Epoch 23  |
|         75000 |    17185.7266 | Layer 1 | Epoch 25  |
|         80000 |    16979.6719 | Layer 1 | Epoch 26  |
|         85000 |    16932.4883 | Layer 1 | Epoch 28  |
|         90000 |    16802.7500 | Layer 1 | Epoch 30  |
|         95000 |    16689.5391 | Layer 1 | Epoch 31  |
|        100000 |    16856.7871 | Layer 1 | Epoch 33  |
|        105000 |    16613.5684 | Layer 1 | Epoch 34  |
|        110000 |    16654.2832 | Layer 1 | Epoch 36  |
|        115000 |    16483.2305 | Layer 1 | Epoch 38  |
|        120000 |    16353.5430 | Layer 1 | Epoch 39  |
|        125000 |    16608.4453 | Layer 1 | Epoch 41  |
|        130000 |    16740.4863 | Layer 1 | Epoch 42  |
|        135000 |    16611.9180 | Layer 1 | Epoch 44  |
|        140000 |    16358.0703 | Layer 1 | Epoch 46  |
|        145000 |    16580.7617 | Layer 1 | Epoch 47  |
|        150000 |    16238.5312 | Layer 1 | Epoch 49  |



| Training Step | Cross Entropy |  Layer  |   Epoch  |
|---------------|---------------|---------|----------|
|             0 |      720.6206 | Layer 2 | Epoch 1  |
|          5000 |      679.0539 | Layer 2 | Epoch 2  |
|         10000 |      654.6862 | Layer 2 | Epoch 4  |
|         15000 |      641.1623 | Layer 2 | Epoch 5  |
|         20000 |      626.0643 | Layer 2 | Epoch 7  |
|         25000 |      614.4658 | Layer 2 | Epoch 9  |
|         30000 |      600.7234 | Layer 2 | Epoch 10  |
|         35000 |      594.7683 | Layer 2 | Epoch 12  |
|         40000 |      586.6774 | Layer 2 | Epoch 13  |
|         45000 |      585.3733 | Layer 2 | Epoch 15  |
|         50000 |      571.7431 | Layer 2 | Epoch 17  |
|         55000 |      568.3016 | Layer 2 | Epoch 18  |
|         60000 |      560.1599 | Layer 2 | Epoch 20  |
|         65000 |      564.1216 | Layer 2 | Epoch 21  |
|         70000 |      548.5504 | Layer 2 | Epoch 23  |
|         75000 |      543.0561 | Layer 2 | Epoch 25  |
|         80000 |      555.9884 | Layer 2 | Epoch 26  |
|         85000 |      541.3663 | Layer 2 | Epoch 28  |
|         90000 |      536.0514 | Layer 2 | Epoch 30  |
|         95000 |      539.6343 | Layer 2 | Epoch 31  |
|        100000 |      537.1501 | Layer 2 | Epoch 33  |
|        105000 |      532.2983 | Layer 2 | Epoch 34  |
|        110000 |      524.1819 | Layer 2 | Epoch 36  |
|        115000 |      530.0859 | Layer 2 | Epoch 38  |
|        120000 |      526.6125 | Layer 2 | Epoch 39  |
|        125000 |      523.8047 | Layer 2 | Epoch 41  |
|        130000 |      513.4188 | Layer 2 | Epoch 42  |
|        135000 |      521.3250 | Layer 2 | Epoch 44  |
|        140000 |      514.4850 | Layer 2 | Epoch 46  |
|        145000 |      512.8197 | Layer 2 | Epoch 47  |
|        150000 |      512.8293 | Layer 2 | Epoch 49  |
Loading images  ../dataset/*.jpeg ['S01']
Number of images: 6200
Number of rows: 256
Number of cols: 256
Dataset array size:  (6200, 256, 256, 1)
Extracting labels from data in  ../dataset/*.jpeg ['S01']
(3100, 256, 256, 1)
(3100,)
(1550, 256, 256, 1)
(1550,)
(1550, 256, 256, 1)
(1550,)
Tensor("ExpandDims_1:0", shape=(250, 1), dtype=int32)
Tensor("ExpandDims:0", shape=(250, 1), dtype=int32)
Tensor("Size:0", shape=(), dtype=int32)
31
Step 0: loss = 3.52 (0.054 sec)
  Num examples: 3000  Num correct: 1571  Error @ 1: 0.4763
  Num examples: 1500  Num correct: 736  Error @ 1: 0.5093
  Num examples: 1500  Num correct: 764  Error @ 1: 0.4907
Step 5000: loss = 2.89 (0.032 sec)
  Num examples: 3000  Num correct: 1964  Error @ 1: 0.3453
  Num examples: 1500  Num correct: 927  Error @ 1: 0.3820
  Num examples: 1500  Num correct: 955  Error @ 1: 0.3633
Step 10000: loss = 2.75 (0.034 sec)
  Num examples: 3000  Num correct: 2085  Error @ 1: 0.3050
  Num examples: 1500  Num correct: 990  Error @ 1: 0.3400
  Num examples: 1500  Num correct: 1029  Error @ 1: 0.3140
Step 15000: loss = 2.72 (0.570 sec)
  Num examples: 3000  Num correct: 2171  Error @ 1: 0.2763
  Num examples: 1500  Num correct: 1020  Error @ 1: 0.3200
  Num examples: 1500  Num correct: 1070  Error @ 1: 0.2867
Step 20000: loss = 2.65 (0.034 sec)
  Num examples: 3000  Num correct: 2331  Error @ 1: 0.2230
  Num examples: 1500  Num correct: 1118  Error @ 1: 0.2547
  Num examples: 1500  Num correct: 1168  Error @ 1: 0.2213
Step 25000: loss = 2.64 (0.028 sec)
  Num examples: 3000  Num correct: 2369  Error @ 1: 0.2103
  Num examples: 1500  Num correct: 1127  Error @ 1: 0.2487
  Num examples: 1500  Num correct: 1174  Error @ 1: 0.2173
Step 30000: loss = 2.63 (0.459 sec)
  Num examples: 3000  Num correct: 2391  Error @ 1: 0.2030
  Num examples: 1500  Num correct: 1132  Error @ 1: 0.2453
  Num examples: 1500  Num correct: 1173  Error @ 1: 0.2180
Step 35000: loss = 2.62 (0.031 sec)
  Num examples: 3000  Num correct: 2361  Error @ 1: 0.2130
  Num examples: 1500  Num correct: 1117  Error @ 1: 0.2553
  Num examples: 1500  Num correct: 1169  Error @ 1: 0.2207
Step 40000: loss = 2.62 (0.030 sec)
  Num examples: 3000  Num correct: 2365  Error @ 1: 0.2117
  Num examples: 1500  Num correct: 1122  Error @ 1: 0.2520
  Num examples: 1500  Num correct: 1169  Error @ 1: 0.2207
Step 45000: loss = 2.60 (0.463 sec)
  Num examples: 3000  Num correct: 2371  Error @ 1: 0.2097
  Num examples: 1500  Num correct: 1126  Error @ 1: 0.2493
  Num examples: 1500  Num correct: 1176  Error @ 1: 0.2160
